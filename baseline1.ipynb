{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advert_id  over\n",
      "advert_industry_inner_1  over\n",
      "advert_industry_inner  over\n",
      "advert_name  over\n",
      "campaign_id  over\n",
      "creative_height  over\n",
      "creative_tp_dnf  over\n",
      "creative_width  over\n",
      "province  over\n",
      "f_channel  over\n",
      "area  over\n",
      "1 cnt_click_of_city 0 s\n",
      "2 cnt_click_of_province 0 s\n",
      "3 cnt_click_of_carrier 0 s\n",
      "4 cnt_click_of_devtype 0 s\n",
      "5 cnt_click_of_make 0 s\n",
      "6 cnt_click_of_model 0 s\n",
      "7 cnt_click_of_nnt 0 s\n",
      "8 cnt_click_of_os 0 s\n",
      "9 cnt_click_of_osv 0 s\n",
      "10 cnt_click_of_adid 0 s\n",
      "11 cnt_click_of_advert_id 0 s\n",
      "12 cnt_click_of_orderid 0 s\n",
      "13 cnt_click_of_advert_industry_inner 0 s\n",
      "14 cnt_click_of_campaign_id 0 s\n",
      "15 cnt_click_of_creative_id 0 s\n",
      "16 cnt_click_of_creative_tp_dnf 0 s\n",
      "17 cnt_click_of_app_cate_id 0 s\n",
      "18 cnt_click_of_f_channel 0 s\n",
      "19 cnt_click_of_inner_slot_id 0 s\n",
      "20 cnt_click_of_creative_type 0 s\n",
      "21 cnt_click_of_creative_width 0 s\n",
      "22 cnt_click_of_creative_height 0 s\n",
      "23 cnt_click_of_creative_is_jump 0 s\n",
      "24 cnt_click_of_creative_has_deeplink 0 s\n",
      "25 cnt_click_of_advert_name 0 s\n",
      "66 ['adid', 'advert_id', 'orderid', 'advert_industry_inner_1', 'advert_industry_inner', 'advert_name', 'campaign_id', 'creative_id', 'creative_type', 'creative_tp_dnf', 'creative_has_deeplink', 'creative_is_jump', 'advert_id_rate', 'advert_industry_inner_1_rate', 'advert_industry_inner_rate', 'advert_name_rate', 'campaign_id_rate', 'creative_height_rate', 'creative_tp_dnf_rate', 'creative_width_rate', 'province_rate', 'f_channel_rate', 'app_cate_id', 'f_channel', 'app_id', 'inner_slot_id', 'city', 'carrier', 'province', 'nnt', 'devtype', 'osv', 'os', 'make', 'model', 'cnt_click_of_city', 'cnt_click_of_province', 'cnt_click_of_carrier', 'cnt_click_of_devtype', 'cnt_click_of_make', 'cnt_click_of_model', 'cnt_click_of_nnt', 'cnt_click_of_os', 'cnt_click_of_osv', 'cnt_click_of_adid', 'cnt_click_of_advert_id', 'cnt_click_of_orderid', 'cnt_click_of_advert_industry_inner', 'cnt_click_of_campaign_id', 'cnt_click_of_creative_id', 'cnt_click_of_creative_tp_dnf', 'cnt_click_of_app_cate_id', 'cnt_click_of_f_channel', 'cnt_click_of_inner_slot_id', 'cnt_click_of_creative_type', 'cnt_click_of_creative_width', 'cnt_click_of_creative_height', 'cnt_click_of_creative_is_jump', 'cnt_click_of_creative_has_deeplink', 'cnt_click_of_advert_name', 'creative_width', 'creative_height', 'hour', 'area', 'period', 'area_rate']\n",
      "one-hot prepared !\n",
      "cv prepared !\n",
      "(1001650, 27176)\n",
      "feature select\n",
      "(1001650, 13588)\n",
      "Fiting...\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[30]\tvalid_0's binary_logloss: 0.427816\tvalid_1's binary_logloss: 0.429104\n",
      "[60]\tvalid_0's binary_logloss: 0.418883\tvalid_1's binary_logloss: 0.420516\n",
      "[90]\tvalid_0's binary_logloss: 0.416379\tvalid_1's binary_logloss: 0.418418\n",
      "[120]\tvalid_0's binary_logloss: 0.415019\tvalid_1's binary_logloss: 0.417603\n",
      "[150]\tvalid_0's binary_logloss: 0.413991\tvalid_1's binary_logloss: 0.417129\n",
      "[180]\tvalid_0's binary_logloss: 0.413139\tvalid_1's binary_logloss: 0.416791\n",
      "[210]\tvalid_0's binary_logloss: 0.412363\tvalid_1's binary_logloss: 0.416558\n",
      "[240]\tvalid_0's binary_logloss: 0.411636\tvalid_1's binary_logloss: 0.416411\n",
      "[270]\tvalid_0's binary_logloss: 0.410973\tvalid_1's binary_logloss: 0.416285\n",
      "[300]\tvalid_0's binary_logloss: 0.410373\tvalid_1's binary_logloss: 0.416201\n",
      "[330]\tvalid_0's binary_logloss: 0.409776\tvalid_1's binary_logloss: 0.416146\n",
      "[360]\tvalid_0's binary_logloss: 0.409208\tvalid_1's binary_logloss: 0.416076\n",
      "[390]\tvalid_0's binary_logloss: 0.408651\tvalid_1's binary_logloss: 0.416052\n",
      "[420]\tvalid_0's binary_logloss: 0.408108\tvalid_1's binary_logloss: 0.416049\n",
      "[450]\tvalid_0's binary_logloss: 0.407574\tvalid_1's binary_logloss: 0.416019\n",
      "[480]\tvalid_0's binary_logloss: 0.407058\tvalid_1's binary_logloss: 0.415992\n",
      "[510]\tvalid_0's binary_logloss: 0.406519\tvalid_1's binary_logloss: 0.415984\n",
      "[540]\tvalid_0's binary_logloss: 0.406028\tvalid_1's binary_logloss: 0.415973\n",
      "[570]\tvalid_0's binary_logloss: 0.405514\tvalid_1's binary_logloss: 0.415917\n",
      "[600]\tvalid_0's binary_logloss: 0.405034\tvalid_1's binary_logloss: 0.415901\n",
      "[630]\tvalid_0's binary_logloss: 0.404552\tvalid_1's binary_logloss: 0.415909\n",
      "[660]\tvalid_0's binary_logloss: 0.404064\tvalid_1's binary_logloss: 0.415933\n",
      "[690]\tvalid_0's binary_logloss: 0.403588\tvalid_1's binary_logloss: 0.415931\n",
      "[720]\tvalid_0's binary_logloss: 0.403115\tvalid_1's binary_logloss: 0.415919\n",
      "Early stopping, best iteration is:\n",
      "[620]\tvalid_0's binary_logloss: 0.404713\tvalid_1's binary_logloss: 0.415894\n",
      "特征重要性不为零的编码特征有 2557 个\n",
      "Fold 0\n",
      "开始进行特征选择计算...\n",
      "共有 2500 个待计算特征\n",
      "Runing...\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[30]\tvalid_0's binary_logloss: 0.431268\tvalid_1's binary_logloss: 0.431059\n",
      "[60]\tvalid_0's binary_logloss: 0.422885\tvalid_1's binary_logloss: 0.423015\n",
      "[90]\tvalid_0's binary_logloss: 0.420562\tvalid_1's binary_logloss: 0.421122\n",
      "[120]\tvalid_0's binary_logloss: 0.419411\tvalid_1's binary_logloss: 0.420479\n",
      "[150]\tvalid_0's binary_logloss: 0.418523\tvalid_1's binary_logloss: 0.420078\n",
      "[180]\tvalid_0's binary_logloss: 0.417816\tvalid_1's binary_logloss: 0.419847\n",
      "[210]\tvalid_0's binary_logloss: 0.417242\tvalid_1's binary_logloss: 0.419731\n",
      "[240]\tvalid_0's binary_logloss: 0.416701\tvalid_1's binary_logloss: 0.419619\n",
      "[270]\tvalid_0's binary_logloss: 0.416193\tvalid_1's binary_logloss: 0.419527\n",
      "[300]\tvalid_0's binary_logloss: 0.415717\tvalid_1's binary_logloss: 0.419449\n",
      "[330]\tvalid_0's binary_logloss: 0.415253\tvalid_1's binary_logloss: 0.419375\n",
      "[360]\tvalid_0's binary_logloss: 0.414796\tvalid_1's binary_logloss: 0.419293\n",
      "[390]\tvalid_0's binary_logloss: 0.414383\tvalid_1's binary_logloss: 0.419282\n",
      "[420]\tvalid_0's binary_logloss: 0.413976\tvalid_1's binary_logloss: 0.419263\n",
      "[450]\tvalid_0's binary_logloss: 0.41356\tvalid_1's binary_logloss: 0.419212\n",
      "[480]\tvalid_0's binary_logloss: 0.413172\tvalid_1's binary_logloss: 0.419211\n",
      "[510]\tvalid_0's binary_logloss: 0.41279\tvalid_1's binary_logloss: 0.419212\n",
      "[540]\tvalid_0's binary_logloss: 0.412402\tvalid_1's binary_logloss: 0.419197\n",
      "[570]\tvalid_0's binary_logloss: 0.412035\tvalid_1's binary_logloss: 0.419182\n",
      "[600]\tvalid_0's binary_logloss: 0.411657\tvalid_1's binary_logloss: 0.419168\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[620]\tvalid_0's binary_logloss: 0.411414\tvalid_1's binary_logloss: 0.419164\n",
      "test mean: 0.2030656034732542\n",
      "前 100 个特征的得分为 0.41916402900838395 而全量得分 0.41589405304040505\n",
      "\n",
      "\n",
      "Runing...\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[30]\tvalid_0's binary_logloss: 0.428803\tvalid_1's binary_logloss: 0.428705\n",
      "[60]\tvalid_0's binary_logloss: 0.420101\tvalid_1's binary_logloss: 0.420437\n",
      "[90]\tvalid_0's binary_logloss: 0.417846\tvalid_1's binary_logloss: 0.418693\n",
      "[120]\tvalid_0's binary_logloss: 0.416612\tvalid_1's binary_logloss: 0.418051\n",
      "[150]\tvalid_0's binary_logloss: 0.415714\tvalid_1's binary_logloss: 0.417694\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7e92782e367f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m#evalsLoss(col[:i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mbest_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mbaseloss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mbest_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-7e92782e367f>\u001b[0m in \u001b[0;36mevalsLoss\u001b[0;34m(cols)\u001b[0m\n\u001b[1;32m    237\u001b[0m         lgb_model.fit(train_csr[train_index][:,cols], train_y[train_index],\n\u001b[1;32m    238\u001b[0m                   eval_set=[(train_csr[train_index][:,cols], train_y[train_index]),\n\u001b[0;32m--> 239\u001b[0;31m                             (train_csr[test_index][:,cols], train_y[test_index])], early_stopping_rounds=100,verbose=30)\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;31m#best_score.append(lgb_model.best_score_['valid_1']['binary_logloss'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;31m#print(best_score)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    724\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    727\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    497\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    206\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1715\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1716\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1718\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "path = './data'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train = pd.read_table(path + '/round1_iflyad_train.txt')\n",
    "test = pd.read_table(path + '/round1_iflyad_test_feature.txt')\n",
    "data = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "data = data.fillna(-1)\n",
    "\n",
    "data['day'] = data['time'].apply(lambda x: int(time.strftime(\"%d\", time.localtime(x))))\n",
    "data['hour'] = data['time'].apply(lambda x: int(time.strftime(\"%H\", time.localtime(x))))\n",
    "data['label'] = data.click.astype(int)\n",
    "data['area'] = data['creative_height'] * data['creative_width']\n",
    "\n",
    "\n",
    "\n",
    "bool_feature = ['creative_is_jump', 'creative_is_download',\n",
    "                'creative_has_deeplink']\n",
    "for i in bool_feature:\n",
    "    data[i] = data[i].astype(int)\n",
    "\n",
    "data['advert_industry_inner_1'] = data['advert_industry_inner'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "data['period'] = data['day']\n",
    "data['period'][data['period']<27] = data['period'][data['period']<27] + 31\n",
    "\n",
    "\n",
    "for feat_1 in ['advert_id','advert_industry_inner_1', 'advert_industry_inner','advert_name','campaign_id', 'creative_height',\n",
    "               'creative_tp_dnf', 'creative_width', 'province', 'f_channel','area']:\n",
    "    gc.collect()\n",
    "    res=pd.DataFrame()\n",
    "    temp=data[[feat_1,'period','click']]\n",
    "    for period in range(27,35):\n",
    "        if period == 27:\n",
    "            count=temp.groupby([feat_1]).apply(lambda x: x['click'][(x['period']<=period).values].count()).reset_index(name=feat_1+'_all')\n",
    "            count1=temp.groupby([feat_1]).apply(lambda x: x['click'][(x['period']<=period).values].sum()).reset_index(name=feat_1+'_1')\n",
    "        else: \n",
    "            count=temp.groupby([feat_1]).apply(lambda x: x['click'][(x['period']<period).values].count()).reset_index(name=feat_1+'_all')\n",
    "            count1=temp.groupby([feat_1]).apply(lambda x: x['click'][(x['period']<period).values].sum()).reset_index(name=feat_1+'_1')\n",
    "        count[feat_1+'_1']=count1[feat_1+'_1']\n",
    "        count.fillna(value=0, inplace=True)\n",
    "        count[feat_1+'_rate'] = round(count[feat_1+'_1'] / count[feat_1+'_all'], 5)\n",
    "        count['period']=period\n",
    "        count.drop([feat_1+'_all', feat_1+'_1'],axis=1,inplace=True)\n",
    "        count.fillna(value=0, inplace=True)\n",
    "        res=res.append(count,ignore_index=True)\n",
    "    print(feat_1,' over')\n",
    "    data = pd.merge(data,res, how='left', on=[feat_1,'period'])\n",
    "\n",
    "\n",
    "ad_cate_feature = ['adid', 'advert_id', 'orderid', 'advert_industry_inner_1', 'advert_industry_inner', 'advert_name',\n",
    "                   'campaign_id', 'creative_id', 'creative_type', 'creative_tp_dnf', 'creative_has_deeplink',\n",
    "                   'creative_is_jump' ,'advert_id_rate','advert_industry_inner_1_rate','advert_industry_inner_rate', 'advert_name_rate',\n",
    "\t\t\t'campaign_id_rate','creative_height_rate','creative_tp_dnf_rate','creative_width_rate' ,'province_rate', 'f_channel_rate']\n",
    "\n",
    "media_cate_feature = ['app_cate_id', 'f_channel', 'app_id', 'inner_slot_id']\n",
    "\n",
    "\n",
    "\n",
    "content_cate_feature = ['city', 'carrier', 'province', 'nnt', 'devtype', 'osv', 'os', 'make', 'model']\n",
    "\n",
    "origin_cate_list = ad_cate_feature + media_cate_feature + content_cate_feature\n",
    "\n",
    "\n",
    "for i in origin_cate_list:\n",
    "    data[i] = data[i].map(dict(zip(data[i].unique(), range(0, data[i].nunique()))))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "label_feature=['adid','advert_id', 'campaign_id', 'creative_id',       \n",
    "       'os', 'carrier']\n",
    "'''\n",
    "label_feature=[ 'city', 'province',  'carrier',\n",
    "               'devtype', 'make', 'model', 'nnt', 'os', 'osv', 'adid',\n",
    "               'advert_id', 'orderid', 'advert_industry_inner', 'campaign_id',\n",
    "               'creative_id', 'creative_tp_dnf', 'app_cate_id', 'f_channel',\n",
    "               'inner_slot_id', 'creative_type', 'creative_width', 'creative_height',\n",
    "               'creative_is_jump','creative_has_deeplink','advert_name']\n",
    "\n",
    "data['cnt']=1\n",
    "col_type = label_feature.copy()\n",
    "n = len(col_type)\n",
    "\n",
    "num = 0\n",
    "#df_feature = pd.DataFrame()\n",
    "for i in range(n):\n",
    "    col_name = \"cnt_click_of_\"+col_type[i]\n",
    "    s = time.time()\n",
    "    se = (data[col_type[i]].map(data[col_type[i]].value_counts())).astype(int)\n",
    "    semax = se.max()\n",
    "    semin = se.min()\n",
    "    data[col_name] = ((se-se.min())/(se.max()-se.min())*100).astype(int).values\n",
    "    #data[col_name]=se.astype(int).values\n",
    "    num+=1\n",
    "    print(num,col_name,int(time.time()-s),'s')\n",
    "'''\n",
    "count_feature=['cnt_click_of_adid', 'cnt_click_of_advert_id',\n",
    "       'cnt_click_of_campaign_id', 'cnt_click_of_creative_id',\n",
    "       'cnt_click_of_os', 'cnt_click_of_carrier']\n",
    "\n",
    "'''\n",
    "count_feature=['cnt_click_of_city',\n",
    "       'cnt_click_of_province', 'cnt_click_of_carrier', 'cnt_click_of_devtype',\n",
    "       'cnt_click_of_make', 'cnt_click_of_model', 'cnt_click_of_nnt',\n",
    "       'cnt_click_of_os', 'cnt_click_of_osv', 'cnt_click_of_adid',\n",
    "       'cnt_click_of_advert_id', 'cnt_click_of_orderid',\n",
    "       'cnt_click_of_advert_industry_inner', 'cnt_click_of_campaign_id',\n",
    "       'cnt_click_of_creative_id', 'cnt_click_of_creative_tp_dnf',\n",
    "       'cnt_click_of_app_cate_id', 'cnt_click_of_f_channel',\n",
    "       'cnt_click_of_inner_slot_id', 'cnt_click_of_creative_type',\n",
    "       'cnt_click_of_creative_width', 'cnt_click_of_creative_height',\n",
    "       'cnt_click_of_creative_is_jump', 'cnt_click_of_creative_has_deeplink',\n",
    "       'cnt_click_of_advert_name']\n",
    "\n",
    "cate_feature = origin_cate_list+count_feature\n",
    "\n",
    "num_feature = ['creative_width', 'creative_height', 'hour'  , 'area', 'period', 'area_rate']\n",
    "\n",
    "feature = cate_feature + num_feature\n",
    "print(len(feature), feature)\n",
    "\n",
    "predict = data[data.label == -1]\n",
    "predict_result = predict[['instance_id']]\n",
    "predict_result['predicted_score'] = 0\n",
    "predict_x = predict.drop('label', axis=1)\n",
    "\n",
    "train_x = data[data.label != -1]\n",
    "train_y = data[data.label != -1].label.values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "del data['click']\n",
    "# 默认加载 如果 增加了cate类别特征 请改成false重新生成\n",
    "if os.path.exists(path + '/feature/base_train_csr.npz') and False:\n",
    "    print('load_csr---------')\n",
    "    base_train_csr = sparse.load_npz(path + '/feature/base_train_csr.npz').tocsr().astype('bool')\n",
    "    base_predict_csr = sparse.load_npz(path + '/feature/base_predict_csr.npz').tocsr().astype('bool')\n",
    "else:\n",
    "    base_train_csr = sparse.csr_matrix((len(train), 0))\n",
    "    base_predict_csr = sparse.csr_matrix((len(predict_x), 0))\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    for feature in cate_feature:\n",
    "        enc.fit(data[feature].values.reshape(-1, 1))\n",
    "\n",
    "        base_train_csr = sparse.hstack((base_train_csr, enc.transform(train_x[feature].values.reshape(-1, 1))), 'csr',\n",
    "                                       'bool')\n",
    "        base_predict_csr = sparse.hstack((base_predict_csr, enc.transform(predict[feature].values.reshape(-1, 1))),\n",
    "                                         'csr',\n",
    "                                         'bool')\n",
    "    print('one-hot prepared !')\n",
    "\n",
    "    cv = CountVectorizer(min_df=20)\n",
    "    for feature in ['user_tags']:\n",
    "        data[feature] = data[feature].astype(str)\n",
    "        cv.fit(data[feature])\n",
    "        base_train_csr = sparse.hstack((base_train_csr, cv.transform(train_x[feature].astype(str))), 'csr', 'bool')\n",
    "        base_predict_csr = sparse.hstack((base_predict_csr, cv.transform(predict_x[feature].astype(str))), 'csr',\n",
    "                                         'bool')\n",
    "    print('cv prepared !')\n",
    "\n",
    "    sparse.save_npz(path + '/feature/base_train_csr.npz', base_train_csr)\n",
    "    sparse.save_npz(path + '/feature/base_predict_csr.npz', base_predict_csr)\n",
    "\n",
    "train_csr = sparse.hstack(\n",
    "    (sparse.csr_matrix(train_x[num_feature]), base_train_csr), 'csr').astype(\n",
    "    'float32')\n",
    "predict_csr = sparse.hstack(\n",
    "    (sparse.csr_matrix(predict_x[num_feature]), base_predict_csr), 'csr').astype('float32')\n",
    "print(train_csr.shape)\n",
    "feature_select = SelectPercentile(chi2, percentile=50)\n",
    "feature_select.fit(train_csr, train_y)\n",
    "train_csr = feature_select.transform(train_csr)\n",
    "predict_csr = feature_select.transform(predict_csr)\n",
    "print('feature select')\n",
    "print(train_csr.shape)\n",
    "\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=62, max_depth=-1, learning_rate=0.05, n_estimators=2000,\n",
    "                           max_bin=425, subsample_for_bin=50000, objective='binary', min_split_gain=0,metric='binary_logloss',\n",
    "                           min_child_weight=5, min_child_samples=10, subsample=0.8, subsample_freq=1,random_state=2018,\n",
    "                           colsample_bytree=1, reg_alpha=3, reg_lambda=5, seed=1000, n_jobs=10, nthread=10, silent=True)\n",
    "\n",
    "print('Fiting...')\n",
    "dev_X, val_X, dev_y, val_y = train_test_split(train_csr, train_y, test_size = 0.2, random_state = 42)\n",
    "lgb_model.fit(dev_X, dev_y,\n",
    "                  eval_set=[(dev_X, dev_y),\n",
    "                            (val_X, val_y)], early_stopping_rounds=100,verbose=30)\n",
    "\n",
    "baseloss = lgb_model.best_score_['valid_1']['binary_logloss']\n",
    "    # 特征重要性\n",
    "se = pd.Series(lgb_model.feature_importances_)\n",
    "se = se[se>0]\n",
    "    ##将特征重要性进行排序\n",
    "\n",
    "col =list(se.sort_values(ascending=False).index)\n",
    "pd.Series(col).to_csv('col_sort_best.csv',index=False)\n",
    "    ##打印出来不为零的特征以及个数\n",
    "print('特征重要性不为零的编码特征有',len(se),'个')\n",
    "n = lgb_model.best_iteration_\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=62, max_depth=-1, learning_rate=0.05, n_estimators=n,\n",
    "                           max_bin=425, subsample_for_bin=50000, objective='binary', min_split_gain=0,metric='binary_logloss',\n",
    "                           min_child_weight=5, min_child_samples=10, subsample=0.8, subsample_freq=1,random_state=2018,\n",
    "                           colsample_bytree=1, reg_alpha=3, reg_lambda=5, seed=1000, n_jobs=10, silent=True)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2018, shuffle=True)\n",
    "best_score = []\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train_csr, train_y)):\n",
    "    print(\"Fold\", index)\n",
    "    def evalsLoss(cols):\n",
    "        print('Runing...')\n",
    "        lgb_model.fit(train_csr[train_index][:,cols], train_y[train_index],\n",
    "                  eval_set=[(train_csr[train_index][:,cols], train_y[train_index]),\n",
    "                            (train_csr[test_index][:,cols], train_y[test_index])], early_stopping_rounds=100,verbose=30)\n",
    "        #best_score.append(lgb_model.best_score_['valid_1']['binary_logloss'])\n",
    "        #print(best_score)\n",
    "        ypre = lgb_model.predict_proba(predict_csr[:,cols], num_iteration=lgb_model.best_iteration_)[:,1]\n",
    "        print('test mean:', ypre.mean())\n",
    "        return (lgb_model.best_score_['valid_1']['binary_logloss'])\n",
    "    \n",
    "    print('开始进行特征选择计算...')\n",
    "    all_num = int(len(se)/100)*100\n",
    "    print('共有',all_num,'个待计算特征')\n",
    "    \n",
    "    break_num = 0\n",
    "    \n",
    "    for i in range(100,all_num,100):\n",
    "        #evalsLoss(col[:i])\n",
    "        best_score.append(evalsLoss(col[:i]))\n",
    "        if best_score[-1]<baseloss:\n",
    "            best_num = i\n",
    "            baseloss = best_score[-1]\n",
    "            break_num+=1\n",
    "        print('前',i,'个特征的得分为',best_score[-1],'而全量得分',baseloss)\n",
    "        print('\\n')\n",
    "        if break_num==14:\n",
    "            break\n",
    "    print('筛选出来最佳特征个数为',best_num,'这下子训练速度终于可以大大提升了')\n",
    "    best_num = len(col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
